<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Yingjie Chen - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Yingjie Chen">
<meta property="og:title" content="Yingjie Chen">


  <link rel="canonical" href="https://github.com/pages/echoanran/chen-yingjie.github.io/">
  <meta property="og:url" content="https://github.com/pages/echoanran/chen-yingjie.github.io/">



  <meta property="og:description" content="School of Computer Science and Technology, Peking University.">









<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/#about-me">About Me</a></li>
          
            <li class="masthead__menu-item"><a href="/#-news">News</a></li>
          
            <li class="masthead__menu-item"><a href="/#-selected-publications">Selected Publications</a></li>
          
            <li class="masthead__menu-item"><a href="/#-honors-and-awards">Honors and Awards</a></li>
          
            <li class="masthead__menu-item"><a href="/#-work-experiences">Work Experiences</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="images/android-chrome-512x512.png" class="author__avatar" alt="Yingjie Chen">
  </div>

  <div class="author__content">
    <h3 class="author__name">Yingjie Chen</h3>
    <p class="author__bio">Peking University</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;">School of Computer Science and Technology, Peking University.</div></li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Beijing, China</li>
      
      
      
      
        <li><a href="mailto:chenyingjie@pku.edu.cn"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/https://github.com/chen-yingjie"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com.hk/citations?user=3fmmfg8AAAAJ&hl=zh-CN"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="mailto:chenyingjie@pku.edu.cn"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
      
      
      
      
      
      
      
      
      
        <a href="https://github.com/https://github.com/chen-yingjie"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com.hk/citations?user=3fmmfg8AAAAJ&hl=zh-CN"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            
<p><span class="anchor" id="about-me"></span></p>

<p>I am Yingjie Chen, an AI Researcher at Alibaba TongYi Lab. I graduated with Ph.D. degree in the school of Computer Science from <a href="https://english.pku.edu.cn/">Peking University</a> in 2024, supervised by Prof. Tao Wang and Prof. Yun Liang. Before that, I received B.S. degree from <a href="https://english.pku.edu.cn/">Peking University</a> in 2019. My research focuses on Computer Vision, Affective Computing and  AI Generative Content. At Alibaba, I’m working on generative AI models, especially cotrollable video generation.</p>

<p>We are now recruiting for Summer Internships, and positions for Research Interns (RI) are continuously open for applications. Welcome to contact me with your CV and research statement!</p>

<h1 id="-news">🔥 News</h1>
<ul>
  <li><em>2025.01</em>:  🎉 Perception-as-Control has been released.</li>
  <li><em>2023.12</em>:  🎉 Trend-Aware-Supervision has been accepted by AAAI 2024.</li>
  <li><em>2023.10</em>:  🎉 EventFormer has been accepted by BMVC 2023.</li>
  <li><em>2023.05</em>:  🎉 One paper has been accepted by SMC 2023.</li>
  <li><em>2022.12</em>:  🎉 CIS has been accepted by AAAI 2022.</li>
  <li><em>2022.07</em>:  🎉 On-Mitigating-Hard-Clusters has been accepted by ECCV 2022.</li>
  <li><em>2022.07</em>:  🎉 SupHCL has been accepted by ACM MM 2022.</li>
  <li><em>2022.04</em>:  🎉 HUMAN has been accepted by IJCAI 2022.</li>
  <li><em>2021.07</em>:  🎉 CaFGraph has been accepted by ACM MM 2021.</li>
  <li><em>2021.07</em>:  🎉 FSNet has been accepted by RA-L, 2021.</li>
  <li><em>2021.07</em>:  🎉 AUPro has been accepted by ICONIP, 2021.</li>
</ul>

<h1 id="-selected-publications">📝 Selected Publications</h1>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">arXiv 2025</div><img src="images/pubs/motion_generation.gif" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/pdf/2501.05020.pdf">Perception-as-Control: Fine-grained Controllable Image Animation with 3D-aware Motion Representation</a></p>

    <p><strong>Yingjie Chen</strong>, Yifang Men, Yuan Yao, Miaomiao Cui, Liefeng Bo</p>

    <p><a href="https://chen-yingjie.github.io/projects/Perception-as-Control/">[Project Page]</a>
<a href="https://arxiv.org/pdf/2501.05020.pdf">[Paper]</a>
<a href="https://github.com/chen-yingjie/Perception-as-Control">[Code]</a></p>
    <ul>
      <li>We introduce 3D-aware motion representation and propose an image animation framework, Perception-as-Control, to achieve fine-grained collaborative motion control. By constructing 3D-aware motion representation based on various user intentions and taking the perception results as motion control signals, Perception-as-Control can be applied to various motion-related video synthesis tasks.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">AAAI 2024</div><img src="images/pubs/trendaware_motivation_compressed.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/pdf/2503.08078.pdf">Trend-Aware Supervision: On Learning Invariance for Semi-supervised Facial Action Unit Intensity Estimation</a></p>

    <p><strong>Yingjie Chen</strong>, Jiarui Zhang, Tao Wang, Yun Liang</p>

    <p><a href="https://arxiv.org/pdf/2503.08078.pdf">[Paper]</a>
<a href="https://github.com/chen-yingjie/Trend_Aware_Supervision">[Code]</a></p>
    <ul>
      <li>We inspect the keyframe-based semi-supervised AU intensity estimation problem and identify the spurious correlation problem as the main challenge. To this end, we propose Trend-Aware Supervision to raise intra-trend and inter-trend awareness during training to learn invariant AU-specifc features.</li>
    </ul>
  </div> 
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ECCV 2022</div><img src="images/pubs/faceclustering_overview.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/pdf/2207.11895.pdf">On Mitigating Hard Clusters for Face Clustering</a></p>

    <p><strong>Yingjie Chen</strong>, Huasong Zhong, Chong Chen, Chen Shen, Jianqiang Huang, Tao Wang, Yun Liang, Qianru Sun</p>

    <p><a href="https://arxiv.org/pdf/2207.11895.pdf">[Paper]</a>
<a href="https://github.com/echoanran/On-Mitigating-Hard-Clusters">[Code]</a></p>
    <ul>
      <li>We inspect face clustering problem and find existing methods failed to identify hard clusters—yielding significantly low recall for small or sparse clusters. To mitigate the issue of small clusters, we introduce NDDe based on the diffusion of neighborhood densities.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ACM MM 2022</div><img src="images/pubs/suphcl_motivation.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://dl.acm.org/doi/10.1145/3503161.3548116">Pursuing Knowledge Consistency: Supervised Hierarchical contrastive Learning for Facial Action Unit Recognition</a></p>

    <p><strong>Yingjie Chen</strong>, Diqi Chen, Tao Wang, Yizhou Wang, Yun Liang</p>

    <p><a href="https://dl.acm.org/doi/10.1145/3503161.3548116">[Paper]</a>
<a href="https://github.com/echoanran/SupHCL">[Code]</a></p>
    <ul>
      <li>We observe that there are three kinds of inherent relations among AUs, which can be treated as strong prior knowledge, and pursuing the consistency of such knowledge is the key to learning subject-consistent representations. To this end, we propose a supervised hierarchical contrastive learning method (SupHCL) for AU recognition to pursue knowledge consistency among different facial images and different AUs.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">AAAI 2022</div><img src="images/pubs/cis_overview.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2204.07935.pdf">Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition</a></p>

    <p><strong>Yingjie Chen</strong>, Huasong Zhong, Chong Chen, Chen Shen, Jianqiang Huang, Tao Wang, Yun Liang, Qianru Sun</p>

    <p><a href="https://arxiv.org/abs/2204.07935.pdf">[Paper]</a>
<a href="https://github.com/echoanran/CIS">[Code]</a></p>
    <ul>
      <li>We formulate subject variant problem in AU recognition using an AU causal diagram to explain the whys and wherefores. Based on our causal diagram, we propose a plug-in causal intervention module, CIS, which could be inserted into advanced AU recognition models for removing the effect caused by confounder Subject.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ACM MM 2021</div><img src="images/pubs/cafgraph_motivation.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475295">CaFGraph: Context-aware Facial Multi-graph Representation for Facial Action Unit Recognition</a></p>

    <p><strong>Yingjie Chen</strong>, Diqi Chen, Yizhou Wang, Tao Wang, Yun Liang</p>

    <p><a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475295">[Paper]</a>
<a href="https://github.com/echoanran/AU_detection_GCN">[Code]</a></p>
    <ul>
      <li>Considering that context is essential to resolve ambiguity in human visual system, modeling context within or among facial images emerges as a promising approach for AU recognition task. To this end, we propose CaFGraph, a novel context-aware facial multi-graph that can model both morphological &amp; muscular-based region-level local context and region-level temporal context.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">RA-L, 2021</div><img src="images/pubs/fsnet_motivation.jpg" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="hhttps://ieeexplore.ieee.org/document/9495228">Cross-Modal Representation Learning for Lightweight and Accurate Facial Action Unit Detection</a></p>

    <p><strong>Yingjie Chen</strong>, Han Wu, Tao Wang, Yizhou Wang, Yun Liang</p>

    <p><a href="https://ieeexplore.ieee.org/document/9495228">[Paper]</a>
<a href="https://github.com/echoanran/AU_detection_optical_flow">[Code]</a></p>
    <ul>
      <li>The dynamic process of facial muscle movement, as the core feature of AU, is yet ignored and rarely exploited by prior studies. Based on such observation, we propose Flow Supervised Module (FSM) to explicitly capture the dynamic facial movement in the form of Flow and use the learned Flow to provide supervision signals for the detection model during the training stage effectively and efficiently.</li>
    </ul>
  </div>
</div>

<h1 id="-honors-and-awards">🎖 Honors and Awards</h1>
<ul>
  <li>UBIQuant Scholarship, PKU, 2023</li>
  <li>Peking University President Scholarship, 2022</li>
  <li>PKU Triple-A Student Pacesetter Award, 2021</li>
  <li>Outstanding Graduates, Beijing, 2019</li>
  <li>Outstanding Graduates, PKU, 2019</li>
  <li>Top 10 Excellent Graduation Thesis, 2019</li>
  <li>PKU Triple-A Student Award, 2018</li>
  <li>National Scholarship, 2018</li>
  <li>PKU Triple-A Student Award, 2017</li>
  <li>Scholarship of Kwang-Hua Education Foundation, 2017</li>
  <li>PKU Triple-A Student Award, 2016</li>
  <li>Scholarship of Tianchuang, 2016</li>
</ul>

<h1 id="-work-experiences">💻 Work Experiences</h1>
<ul>
  <li><em>2024.7 - Present</em>, AI Reasercher at Alibaba TongYi Lab.</li>
  <li><em>2023.7 - 2023.10</em>, Reasercher Intern at Tencent.</li>
  <li><em>2023.2 - 2023.6</em>, Reasercher Intern at Apple Inc..</li>
  <li><em>2021.5 - 2022.7</em>, Reasercher Intern at Alibaba Damo Academy.</li>
  <li><em>2020.11 - 2021.2</em>, Reasercher Intern at SenseTime.</li>
  <li><em>2018.8 - 2019.2</em>, Reasercher Intern at MSRA.</li>
</ul>

          </section>
        </div>
      </article>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://cdn.jsdelivr.net/gh/echoanran/chen-yingjie.github.io@'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            var totalCitation = data['citedby']
            document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>


  </body>
</html>
