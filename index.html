<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yingjie Chen</title>

    <meta name="author" content="Yingjie Chen">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yingjie Chen
                </p>
                <p>I am an AI Researcher at Alibaba TongYi Lab. I graduated with Ph.D. degree in the school of Computer Science from <a href="https://english.pku.edu.cn/">Peking University</a> in 2024 and B.S. degree from Peking University in 2019. My research focuses on Computer Vision and Affective Computing. At Alibaba, I'm working on generative AI models, especially cotrollable video generation.
                </p>
                <p>
                  We are now recruiting for Summer Internships, and positions for Research Interns (RI) are continuously open for applications. Welcome to contact me with your CV and research statement!
                </p>

                <p style="text-align:center">
                  <a href="mailto:chenyingjie@pku.edu.cn">Email</a> &nbsp/&nbsp
                  <a href="https://scholar.google.com.hk/citations?user=3fmmfg8AAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                  <a href="https://github.com/echoanran">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:32%;max-width:32%">
                <a href="images/yingjiechen_compressed.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/yingjiechen_compressed.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications
                    <span style="font-size: 55%;">(* denotes equal contribution)</span>
                </h2>

              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <!--  TrendAware  -->
            <tr onmouseout="trendaware_stop()" onmouseover="trendaware_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='trendaware_image'><video  width=100% muted autoplay loop>
                  <source src="images/trendaware_motivation_compressed.png" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/trendaware_motivation_compressed.png' width=100%>
                </div>
                <script type="text/javascript">
                  function trendaware_start() {
                    document.getElementById('trendaware_image').style.opacity = "1";
                  }
        
                  function trendaware_stop() {
                    document.getElementById('trendaware_image').style.opacity = "0";
                  }
                  trendaware_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27803">
                  <span class="papertitle">Trend-Aware Supervision: On Learning Invariance for Semi-supervised Facial Action Unit Intensity Estimation</span>
                </a>
                <br>
                <strong>Yingjie Chen*</strong>,
                Jiarui Zhang*,
                Tao Wang,
                Yun Liang
                <br>
                <em>ECCV, 2022</em>

                <br>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27803">arXiv</a>
                /
                <a href="https://github.com/echoanran/Trend_Aware_Supervision">code</a>
                <p></p>
                <p>
                We inspect the keyframe-based semi-supervised AU intensity estimation problem and identify the spurious correlation problem as the main challenge. To this end, we propose Trend-Aware Supervision to raise intra-trend and inter-trend awareness during training to learn invariant AU-specifc features.
                </p>
              </td>
            </tr>

            <!--  FaceClustering  -->
            <tr onmouseout="faceclustering_stop()" onmouseover="faceclustering_start()" bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='faceclustering_image'><video  width=100% muted autoplay loop>
                  <source src="images/faceclustering_cmp_compressed.png" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/faceclustering_cmp_compressed.png' width=100%>
                </div>
                <script type="text/javascript">
                  function faceclustering_start() {
                    document.getElementById('faceclustering_image').style.opacity = "1";
                  }
        
                  function faceclustering_stop() {
                    document.getElementById('faceclustering_image').style.opacity = "0";
                  }
                  faceclustering_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2207.11895.pdf">
                  <span class="papertitle">On Mitigating Hard Clusters for Face Clustering</span>
                </a>
                <br>
                <strong>Yingjie Chen*</strong>,
                Huasong Zhong*,
                Chong Chen,
                Chen Shen,
                Jianqiang Huang,
                Tao Wang,
                Yun Liang,
                Qianru Sun
                <br>
                <em>ECCV, 2022</em> <font color="red"><strong> &nbsp (Oral Presentation, Top 2.3%)</strong></font>

                <br>
                <a href="https://arxiv.org/pdf/2207.11895.pdf">arXiv</a>
                /
                <a href="https://github.com/echoanran/On-Mitigating-Hard-Clusters">code</a>
                <p></p>
                <p>
                We inspect face clustering problem and find existing methods failed to identify hard clustersâ€”yielding significantly low recall for small or sparse clusters. To mitigate the issue of small clusters, we introduce NDDe based on the diffusion of neighborhood densities.
                </p>
              </td>
            </tr>

            <!-- SupHCL  -->
            <tr onmouseout="suphcl_stop()" onmouseover="suphcl_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='suphcl_image'><video  width=100% muted autoplay loop>
                  <source src="images/suphcl_motivation.png" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/suphcl_motivation.png' width=100%>
                </div>
                <script type="text/javascript">
                  function suphcl_start() {
                    document.getElementById('suphcl_image').style.opacity = "1";
                  }
        
                  function suphcl_stop() {
                    document.getElementById('suphcl_image').style.opacity = "0";
                  }
                  suphcl_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/10.1145/3503161.3548116">
                  <span class="papertitle">Pursuing Knowledge Consistency: Supervised Hierarchical contrastive Learning for Facial Action Unit Recognition</span>
                </a>
                <br>
                <strong>Yingjie Chen</strong>,
                Diqi Chen,
                Tao Wang,
                Yizhou Wang,
                Yun Liang,
                <br>
                <em>ACM MM, 2022</em>

                <br>
                <a href="https://dl.acm.org/doi/10.1145/3503161.3548116">arXiv</a>
                /
                <a href="https://github.com/echoanran/SupHCL">code</a>
                <p></p>
                <p>
                We observe that there are three kinds of inherent relations among AUs, which can be treated as strong prior knowledge, and pursuing the consistency of such knowledge is the key to learning subject-consistent representations. To this end, we propose a supervised hierarchical contrastive learning method (SupHCL) for AU recognition to pursue knowledge consistency among different facial images and different AUs. 
                </p>
              </td>
            </tr>

            <!--  CIS  -->
            <tr onmouseout="cis_stop()" onmouseover="cis_start()" bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='cis_image'><video  width=100% muted autoplay loop>
                  <source src="images/cis_motivation.png" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/cis_motivation.png' width=100%>
                </div>
                <script type="text/javascript">
                  function cis_start() {
                    document.getElementById('cis_image').style.opacity = "1";
                  }
        
                  function cis_stop() {
                    document.getElementById('cis_image').style.opacity = "0";
                  }
                  cis_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2204.07935.pdf">
                  <span class="papertitle">On Mitigating Hard Clusters for Face Clustering</span>
                </a>
                <br>
                <strong>Yingjie Chen</strong>,
                Diqi Chen,
                Tao Wang,
                Yizhou Wang,
                Yun Liang,
                <br>
                <em>AAAI, 2022</em> <font color="red"><strong> &nbsp (Oral Presentation)</strong></font>

                <br>
                <a href="https://arxiv.org/abs/2204.07935.pdf">arXiv</a>
                /
                <a href="https://github.com/echoanran/CIS">code</a>
                <p></p>
                <p>
                We formulate subject variant problem in AU recognition using an AU causal diagram to explain the whys and wherefores. Based on our causal diagram, we propose a plug-in causal intervention module, CIS, which could be inserted into advanced AU recognition models for removing the effect caused by confounder Subject.
                </p>
              </td>
            </tr>

            <!--  CaFGraph  -->
            <tr onmouseout="cis_stop()" onmouseover="cis_start()" bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='cafgraph_image'><video  width=100% muted autoplay loop>
                  <source src="images/cafgraph_motivation.png" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/cafgraph_motivation.png' width=100%>
                </div>
                <script type="text/javascript">
                  function cis_start() {
                    document.getElementById('cafgraph_image').style.opacity = "1";
                  }
        
                  function cis_stop() {
                    document.getElementById('cafgraph_image').style.opacity = "0";
                  }
                  cis_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475295">
                  <span class="papertitle">CaFGraph: Context-aware Facial Multi-graph Representation for Facial Action Unit Recognition</span>
                </a>
                <br>
                <strong>Yingjie Chen</strong>,
                Diqi Chen,
                Yizhou Wang,
                Tao Wang,
                Yun Liang,
                <br>
                <em>ACM MM, 2021</em> <font color="red"><strong> &nbsp (Oral Presentation)</strong></font>

                <br>
                <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475295">arXiv</a>
                /
                <a href="https://github.com/echoanran/AU_detection_GCN">code</a>
                <p></p>
                <p>
                Considering that context is essential to resolve ambiguity in human visual system, modeling context within or among facial images emerges as a promising approach for AU recognition task. To this end, we propose CaFGraph, a novel context-aware facial multi-graph that can model both morphological & muscular-based region-level local context and region-level temporal context. 
                </p>
              </td>
            </tr>

            <!--  FSNet  -->
            <tr onmouseout="cis_stop()" onmouseover="cis_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='fsnet_image'><video  width=100% muted autoplay loop>
                  <source src="images/fsnet_motivation.png" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/fsnet_motivation.png' width=100%>
                </div>
                <script type="text/javascript">
                  function cis_start() {
                    document.getElementById('fsnet_image').style.opacity = "1";
                  }
        
                  function cis_stop() {
                    document.getElementById('fsnet_image').style.opacity = "0";
                  }
                  cis_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/9495228">
                  <span class="papertitle">Cross-Modal Representation Learning for Lightweight and Accurate Facial Action Unit Detection</span>
                </a>
                <br>
                <strong>Yingjie Chen</strong>,
                Han Wu,
                Tao Wang,
                Yizhou Wang,
                Yun Liang,
                <br>
                <em>IEEE Robotics Autom. Lett. 2021</em> 

                <br>
                <a href="https://ieeexplore.ieee.org/document/9495228">arXiv</a>
                /
                <a href="https://github.com/echoanran/AU_detection_optical_flow">code</a>
                <p></p>
                <p>
                The dynamic process of facial muscle movement, as the core feature of AU, is yet ignored and rarely exploited by prior studies. Based on such observation, we propose Flow Supervised Module (FSM) to explicitly capture the dynamic facial movement in the form of Flow and use the learned Flow to provide supervision signals for the detection model during the training stage effectively and efficiently.
                </p>
              </td>
            </tr>

          </tbody></table>

          <br>
          <br>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
              <tr>
                  <td>
                      <h2>Awards</h2>
                  </td>
              </tr>
          </tbody></table>

          <br>
          <br>
          <table width="100%" align="center" border="0" cellpadding="10">
              <tr>
                  <td>
                      UBIQuant Scholarship, PKU, 2023
                      <br>
                      <br>
                      Peking University President Scholarship, 2022
                      <br>
                      <br>
                      PKU Triple-A Student Pacesetter Award, 2021
                      <br>
                      <br>
                      Outstanding Graduates, Beijing, 2019
                      <br>
                      <br>
                      Outstanding Graduates, PKU, 2019
                      <br>
                      <br>
                      Top 10 Excellent Graduation Thesis, 2019
                      <br>
                      <br>
                      PKU Triple-A Student Award, 2018/2017/2016
                      <br>
                      <br>
                      National Scholarship, 2018
                      <br>
                      <br>
                      Scholarship of Kwang-Hua Education Foundation, 2017
                      <br>
                      <br>
                      Scholarship of Tianchuang, 2016
                      <br>
                      <br>
                  </td>
              </tr>
          </table>
          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
              <tr>
                  <td>
                      <h2>Experiences</h2>
                  </td>
              </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="10">
              <tr>
                  <td>
                      Researcher at TongYi Lab, Alibaba Group
                      <span style="float:right;">Jul. 2024 - Present</span>
                      <br>
                      <br>
                      Doctor's Degree, Peking University, Beijing, China
                      <span style="float:right;">Sep. 2019 - Jul. 2024</span>
                      <br>
                      <br>
                      Bachelor's Degree, Peking University, Beijing, China
                      <span style="float:right;">Sep. 2015 - Jun. 2019</span>
                      
                  </td>
              </tr>
          </table>
          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
              <tbody>
              <tr>
                  <td>
                      <h2>Contact Me</h2>
                  </td>
              </tr>
              </tbody>

              <table width="100%" align="center" border="0" cellpadding="10">
                  <tr>
                      <td>
                          Email: chenyingjie@pku.edu.cn
                      </td>
                  </tr>
              </table>

              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                  <tbody>
                  <tr>
                      <td style="padding:0px">
                          <br>
                          <p style="text-align:right;font-size:small;">
                          This page template is borrowed from <a href="https://github.com/jonbarron/jonbarron_website">&#10025;</a>.
                          </p>
                      </td>
                  </tr>
                  </tbody>
              </table>
          </table>
            
  </body>
</html>